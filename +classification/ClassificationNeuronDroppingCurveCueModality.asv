% TO DO
% - write as a function 
% - do with at least 100 repetitions (rip)
% - figure out the ideal number of PC? (probably first, let's do it with like 5 runs at define it, RIP too though)
% - do proper figures
% 
% all in all, gives interesting results that are kinda similar though, so not bad


%I will be able to use this code to perform classification
%but I have to change a few things: 
%1) use not per trial dataset, use the one combining units as "features"
%2) average firing rate over phases -> 40 times x classification
%   - is this an issue...? I will be classifying 40 trials with more than
%   40 features which is kinda weird, how do I augment the number of trials
%   I have? 
%3) Maybe I should do PCA and use 
clc
clear all

subject = 'n1';

unit_region = 'PPC'; 
num_of_iterations = 2;

Cues = {'Image', 'Auditory', 'Written'};
CueCount  = length(Cues);
testing_phase = 2; %find(ismember(data.task.phaseNames, testing_phase_name));

for cues  = 1:CueCount

    cueType = Cues{cues};
    load(['C:\Users\Sarah\Documents\Andersen Lab\Data\DataTmp\' subject '_2019_aligned_correctly\Table_sorting_aligned_thr_-4.5_' cueType 'CuePerModality_2s.mat'])

    GraspsToTest = {'Lateral', 'WritingTripod', 'MediumWrap', 'PalmarPinch','Sphere3Finger'}; 

    classifier = 'diaglinear';

    %logical array for unit region
    if strcmp(unit_region, 'all')
        region_idx = ones(size(Go_data.nsp));

    elseif strcmp(unit_region, 'AIP')|| strcmp(unit_region, 'SMG') || strcmp(unit_region, 'M1')
       region_idx = (Go_data.nsp == 1); 
    elseif strcmp(unit_region, 'BA5') || strcmp(unit_region, 'PMV') || strcmp(unit_region, 'PPC')
       region_idx = (Go_data.nsp == 2); 
    elseif strcmp(unit_region, 'S1X') 
        region_idx = (Go_data.nsp == 3);    
    end

    DataAll = table2cell(Go_data(region_idx,ismember(Go_data.Properties.VariableNames, GraspsToTest)));

    TimePhaseLabels = Go_data.time_phase_labels{1};

    %transform data into 40 trials x NBR features matrix 
    DataPhase = cell2mat(cellfun(@(x) mean(x(TimePhaseLabels == testing_phase,:)), DataAll,'UniformOutput', false))';
    DataPhaseAll{cues} = DataPhase;
end


%%%% CHOOSE HERE %%%%%

%DataPhase1 = DataPhaseAll{1}; %Image
%DataPhase1 = DataPhaseAll{2}; %Auditory
DataPhase1 = DataPhaseAll{3};  %Written
flagPCA = false; 

if isequal(DataPhase1,DataPhaseAll{1})

    Data1Name = 'Image';
    DataPhase2 = DataPhaseAll{2};    
    Data2Name = 'Auditory';

    DataPhase3 = DataPhaseAll{3};
    Data3Name = 'Written';

elseif isequal(DataPhase1,DataPhaseAll{2}) 

    Data1Name = 'Auditory';
    DataPhase2 = DataPhaseAll{1};
    Data2Name = 'Image';

    DataPhase3 = DataPhaseAll{3};
    Data3Name = 'Written';

elseif isequal(DataPhase1,DataPhaseAll{3})

    Data1Name = 'Written';
    DataPhase2 = DataPhaseAll{2};
    Data2Name = 'Auditory';
    DataPhase3 = DataPhaseAll{1};
    Data3Name = 'Image';    

end 



%testing_phase = find(ismember(data.task.phaseNames, testing_phase_name));

%get labels and data from corresponding phase
%for repetition = 1:100
LabelsNumber = cell2mat(cellfun(@(x) preproc.image2class_simple(x), GraspsToTest, 'UniformOutput', false));
LabelsAll = reshape(repmat(LabelsNumber,8,1), [],1);

max_feature_number = size(DataPhase,2);
result = cell(1,num_of_iterations);
k = 8;
% result2 = nan(1,num_of_iterations);
% parfor rep = 1:num_of_iterations
%     % processing....
%     result{rep} = true; % result of processing this iteration
%     result2(rep) = 1;
% end
flagPCA = true; 
tic
%parfor rep = 1:num_of_iterations

for rep = 1:num_of_iterations %for debugging
    %err = nan(k,num_phases,max_feature_number);
    err = nan(k,CueCount,  max_feature_number);
    disp(rep)
    for num_features = 1:max_feature_number
        
        %completely randomize the available features each time
        features_rand = randperm(size(DataPhase1,2));
        %keep 1 feature in loop 1, 2 features in loop 2, etc. 
        features_idx = features_rand(1:num_features);
        %select the data with the selected features (= units)
        data_cv = DataPhase1(:,features_idx);
        data_2 = DataPhase2(:,features_idx);
        data_3 = DataPhase3(:,features_idx);
        %neuralData_per_cell = arrayfun(@(x)data_cv(phase_info.trials{testing_phase}==x, :), 1:phase_info.trials{testing_phase}(end), 'UniformOutput', false);
        %labels_cell = arrayfun(@(x)labels(phase_info.trials{testing_phase}==x), 1:phase_info.trials{testing_phase}(end), 'UniformOutput', false);
        
        cv = cvpartition(size(DataPhase1,1), 'KFold', k);
        cv = repartition(cv); %pas forcement utile
        
        for runNbr = 1:cv.NumTestSets %iterate over different data partitions
            %find training and testing data and labels indexes
            trIdx = find(cv.training(runNbr));
            teIdx = find(cv.test(runNbr));
            
            labels_train = LabelsAll(trIdx);
            
            labels_test = LabelsAll(teIdx);
            
            DataTraining = data_cv(trIdx,:);
            
            DataTesting1 = data_cv(teIdx,:);
            DataTesting2 = data_2(teIdx,:);
            DataTesting3 = data_3(teIdx,:); 
            
            if flagPCA
                     [coeff, score, latent,~, explained] = pca(DataTraining);
                      %for v = 1:length(explained)
                      %    variance_c(v) = sum(explained(1:v));
                      %end 
                      idx_90 = 30; %find(variance_c>90,1);
                      %disp(['Pca taken ' num2str(idx_90) ])
                      if idx_90 < size(DataTraining,2)
                        DataTraining = DataTraining*coeff(:,1:idx_90);
                        DataTesting1 = DataTesting1*coeff(:,1:idx_90);
                        DataTesting2 = DataTesting2*coeff(:,1:idx_90);
                        DataTesting3 = DataTesting3*coeff(:,1:idx_90);
                      else
                        DataTraining = DataTraining*coeff;
                        DataTesting1 = DataTesting1*coeff;
                        DataTesting2 = DataTesting2*coeff;
                        DataTesting3 = DataTesting3*coeff;
                      end 
                      %KeepPC = [KeepPC, idx_90];
            end 
            
            model = fitcdiscr(DataTraining, labels_train, 'DiscrimType', classifier);
            %size(DataTraining)

            %predicted_labels1 = predict(model, DataTesting1); 
            err(runNbr,1 ,num_features) = classification.classerror(labels_test, predict(model, DataTesting1));
            err(runNbr,2 ,num_features) = classification.classerror(labels_test, predict(model, DataTesting2));
            err(runNbr,3 ,num_features) = classification.classerror(labels_test, predict(model, DataTesting3));
 
        end 
        
    end 
    result{rep} = err;

end 

toc
            %for current_phase = 1:num_phases
%                 
%                 if current_phase == testing_phase
%                     predicted_labels = predict(model2, data_testphase);
%                     
%                     err(runNbr,current_phase,num_features) = classerror(labels_test, predicted_labels);
%                     %err2(run, current_phase, num_features) = classerror(labels_test, predicted_labels);
%                     
%                 else
%                     data_t = phase_info.data{current_phase};
%                     data_t = data_t(:,logical(all_units_low_FR_removed)); % we have to put the correct units, and remove the ones that don-t have enough 
%                                                                           % otherwise bad classification
%                     data_t = data_t(:,features_idx);
%                    
%                     labels_t = phase_info.labels{current_phase};
%                     
%                     neuralData_per_cell_t = arrayfun(@(x)data_t(phase_info.trials{current_phase}==x, :), 1:phase_info.trials{current_phase}(end), 'UniformOutput', false);
%                     labels_cell_t = arrayfun(@(x)labels_t(phase_info.trials{current_phase}==x), 1:phase_info.trials{current_phase}(end), 'UniformOutput', false);
%                     
%                     labels_test_t = labels_cell_t(teIdx);
%                     labels_test_t = cat(1,labels_test_t{:});
%                     %test_labels = cat(1,labels_test_t{:});
%                     
%                     data_testphase_t = neuralData_per_cell_t(teIdx);
%                     data_testphase_t =  cat(1, data_testphase_t{:});
%                     
%                     predicted_labels = predict(model2, data_testphase_t);
%                     err(runNbr,current_phase,num_features) = classerror(labels_test_t, predicted_labels);
%                     %  result{rep} = err2;
%                 end
                %err(run, current_phase, num_features) = classerror(test_labels, predicted_labels);
                
               % err(run, current_phase, num_features) = classerror(test_labels, predicted_labels);
                %   err(:,:,:,rep) = err2;
                
            %end
            %sliding_window_classification(data, model2, 'bins', 10, 'step',5);
            
        %end
    %end
    
%end

Err1 = cell2mat(cellfun(@(x) squeeze(x(:,1,:)),result', 'UniformOutput', false));
Err2 = cell2mat(cellfun(@(x) squeeze(x(:,2,:)),result', 'UniformOutput', false));
Err3 = cell2mat(cellfun(@(x) squeeze(x(:,3,:)),result', 'UniformOutput', false));


ColorModalities = utile.get_color_rgb_codes({Data1Name, Data2Name, Data3Name});
figure(); 

plot(1-mean(Err1), 'Color', ColorModalities{1});
hold on 
plot(1-mean(Err2),'Color', ColorModalities{2});
hold on 

plot(1-mean(Err3),'Color', ColorModalities{3});
title([subject  ' - ' unit_region ' - Training with ' Data1Name ' Modality'] )
disp('here')



%end
%end
%One error per phase
save_name = [Figure_labels.subject 'NeuronDroppingCurve_3_grasps_MODEL_CUE_PHASE_Region_' Figure_labels.unit_region '_' date '.mat'];
%save('p3_NeuronDroppingCurve_AIPandPMV_02_11.mat', 'result', 'Figure_labels')
%save(save_name,  'result', 'Figure_labels')
%%
PhaseNames = {' ITI  ', ' Image Cue ', ' Delay ' , ' Action '};
err = cat(4,result{:});
colors_plot = utile.get_color_rgb_codes(PhaseNames);
Fontsize = 15; 
%err = [cross validation x current_phase x number of features x repetition]
figure();
for phase = 1:size(err  ,2)
    
err_specific = squeeze(err(:,phase,:,:));

%first average over the 4 cross validations
%mean_cross_validations= squeeze(mean(err,1));
mean_specific = squeeze(mean(err_specific));
ci_specific = bootci(1e3, {@nanmean, mean_specific'});

mean_specific = mean(mean_specific,2); 

err_ci_specific(1,:) = ci_specific(2,:) - mean_specific'; 
err_ci_specific(2,:) = mean_specific' - ci_specific(1,:); 

%Check that ci is correct in errorbar
% figure()
% hold on 
% plot(abs(1-mean_specific)*100,'b')
% hold on 
% plot(abs(1-ci_specific(1,:))*100, 'r*')
% hold on 
% plot(abs(1-ci_specific(2,:))*100,'g*')
% 
%figure();
hold on 
%e = errorbar(1:length(mean_err),abs(1 -mean_err)*100, err_ci1*100, err_ci2*100);
e(phase) =errorbar(1:length(mean_specific),abs(1 -mean_specific)*100,err_ci_specific(1,:)*100, err_ci_specific(2,:)*100);
hold on 
%for i = 1:length(e)
e(phase).Color = colors_plot{phase};
%end 
%plot(abs(1-mean_err)*100,'.');
end 
title([Figure_labels.subject '  - Neuron Dropping Curve' ],'FontSize' , Fontsize);
    %- Training over Action phase'],'FontSize', Fontsize);
xlabel('Number of Neurons','FontSize', Fontsize);
ylabel('Mean Classification Accuracy [%]','FontSize', Fontsize);
legend(PhaseNames, 'FontSize', Fontsize);






% figure();
% 
% %first average over the 4 cross validations
% mean_cross_validations= squeeze(mean(err,1));
% %generate standard deviation over the 1000 iterations
% mean_err = squeeze(mean(mean_cross_validations,3))';
% err_ci = bootci(1e3, {@nanmean, mean_err'});
% std_dev = std(mean_cross_validations, [],3)';
% 
% 
% %figure();
% errorbar(abs(1 -mean_err)*100, std_dev*100);
% %plot(abs(1-mean_err)*100,'.');
% title([Figure_labels.unit_region ' Neuron Dropping Curve']);
% xlabel('Number of Neurons');
% ylabel('Mean Classification Accuracy [%]');
% legend(LegendNames);



% color = {'rp','bp','mp','bp'};
% chance_level = 1/(length(unique(labels)))*100;
% for kk = 4 %1:size(err,3)
%   % subplot(2,2,kk)
%     err_data = squeeze(err(:,:,kk,:));
%     err_data = permute(err_data, [1 3 2]);
%     err_data = reshape(err_data, [], size(err_data,1), 1);
%   %  mean_err = squeeze(mean(err(:,:,kk)))
%     mean_err = mean(err_data)
%     %blub = mean(squeeze(std(err)),2);
%     std_deviation = std(err_data)
%  %   blubi = plot(mean_err*100, color{kk});
%     hold on
%     ylim([0 80]);
%     xlim([0.8 4.2]);
%     errorbar(abs(1 -mean_err)*100, std_deviation*100, color{kk});
%     hold on
%     xL = get(gca,'XLim');
%    % l = line([1:size(err,2)],repmat(chance_level, [1, size(err,2)]),xL,'Color','k','LineStyle','--','Linewidth', 2);
%     l = line([0.8 4.2],[chance_level,chance_level],xL,'Color','r','LineStyle','--','Linewidth', 1);
%
%     legend(l, 'Chance level')
%     legend boxoff                   % Hides the legend's axes (legend border and background)
%
%     set(gca,'xtick',[1:size(err,2)],'xticklabel',PhaseNames)
%     xlabel('Phase');
%     ylabel('Classification Accuracy [%]');
%     truc = [data.region ': Classification using' PhaseNames{kk},'phase'];
%     title(truc);
% end
% %%
%
%
% %sliding window for each phase, obtaining several errors per phase
%
%
%
%




